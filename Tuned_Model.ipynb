{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat=list(df.columns.values)\n",
    "print feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.remove('id')\n",
    "feat.remove('loanstatus')\n",
    "feat.remove('train_flg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary manually parameter tuning based on stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#Kfolds = StratifiedKFold(df_all['loan_status'], n_folds = 3, shuffle=True, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.query(\"train_flg == 1\")\n",
    "df_test =df.query(\"train_flg == 0\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df_train[feat], df_train.loanstatus, \n",
    "                                                      test_size=0.3, random_state=2016, stratify = df_train.loanstatus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = df_test[feat], df_test.loanstatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, y_train, missing = np.NAN)\n",
    "dvalid = xgb.DMatrix(X_valid, y_valid, missing = np.NAN)\n",
    "dtest = xgb.DMatrix(X_test, y_test, missing = np.NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\": \"binary:logistic\", \n",
    "          \"booster\" : \"gbtree\", \n",
    "          \"eta\": 0.05, \n",
    "          \"max_depth\": 6, \n",
    "          \"subsample\": 0.632, \n",
    "          \"colsample_bytree\": 0.7,\n",
    "          #\"colsample_bylevel\": 0.6,\n",
    "          \"silent\": 1, \n",
    "          \"seed\": 1441, \n",
    "          \"eval_metric\": \"auc\",\n",
    "          #\"gamma\": 1, \n",
    "          \"min_child_weight\": 5} # 74453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "num_boost_round = 1500\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist,\\\n",
    "  early_stopping_rounds= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import linear_model, datasets\n",
    "import pylab as pl\n",
    "def draw_ROC(model, dtrain, dvalid, dtest, y_train, y_valid, y_test ):\n",
    "    probas_ = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n",
    "    probas_1 = model.predict(dtrain, ntree_limit=model.best_ntree_limit)\n",
    "    probas_2 = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, probas_)\n",
    "    fpr_1, tpr_1, thresholds_1 = roc_curve(y_train, probas_1)\n",
    "    fpr_2, tpr_2, thresholds_2 = roc_curve(y_test, probas_2)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_auc_1 = auc(fpr_1, tpr_1)\n",
    "    roc_auc_2 = auc(fpr_2, tpr_2)\n",
    "    print \"Area under the ROC curve - validation: %f\" % roc_auc\n",
    "    print \"Area under the ROC curve - train: %f\" % roc_auc_1\n",
    "    print \"Area under the ROC curve - test: %f\" % roc_auc_2\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(fpr, tpr, label='ROC curve - test(AUC = %0.2f)' % roc_auc, color='r')\n",
    "    plt.plot(fpr_1, tpr_1, label='ROC curve - train (AUC = %0.2f)' % roc_auc_1, color='b')\n",
    "    plt.plot(fpr_2, tpr_2, label='ROC curve - train (AUC = %0.2f)' % roc_auc_2, color='g')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC for lead score model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_ROC(gbm, dtrain, dvalid, dtest, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print y_pred.max(), y_pred.min(), y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance=gbm.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.DataFrame(importance.items(), columns=['feature', 'fscore'])\n",
    "df_importance['fscore'] = df_importance['fscore'] / df_importance['fscore'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_importance.sort_values(['fscore'], ascending=False, inplace=True)\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32, 32))\n",
    "# df_importance.plot()\n",
    "df_importance[:20].plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "plt.gcf().savefig('feature_importance_xgb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_importance = df_importance.query(\"feature=='grade'\")\n",
    "grade_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgrade_importance = df_importance.query(\"feature=='subgrade'\")\n",
    "subgrade_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrate_importance = df_importance.query(\"feature=='intrate'\")\n",
    "intrate_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance.query(\"feature=='loanamnt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32, 32))\n",
    "# df_importance.plot()\n",
    "df_importance.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "train_x = df_train[feat]\n",
    "train_y = df_train.loanstatus\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y, missing = np.NAN)  \n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma):\n",
    "    params = dict()\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['eta'] = 0.05\n",
    "    params['max_depth'] = int(max_depth )   \n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['verbose_eval'] = False \n",
    "\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain,\n",
    "                       num_boost_round=100000,\n",
    "                       nfold=3,\n",
    "                       metrics={'auc'},\n",
    "                       seed=1234,\n",
    "                       callbacks=[xgb.callback.early_stop(50)])\n",
    "    print(cv_result)\n",
    "\n",
    "    return cv_result['test-auc-mean'].max()\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(xgb_evaluate, \n",
    "                             {'max_depth': (4, 8),\n",
    "                              'min_child_weight': (0, 20),\n",
    "                              'colsample_bytree': (0.2, 0.8),\n",
    "                              'subsample': (0.5, 1),\n",
    "                              'gamma': (0, 2)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "xgb_BO.maximize(init_points=5, n_iter=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning results\n",
    "xgb_BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])\n",
    "xgb_BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])\n",
    "xgb_BO_scores = xgb_BO_scores.sort_values(by='score',ascending=False)\n",
    "xgb_BO_scores.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# parameter set 1\n",
    "params = {'objective': 'binary:logistic'\n",
    "                  , 'booster': 'gbtree'\n",
    "                  , 'eta': 0.01\n",
    "                  , 'max_depth': 4     \n",
    "                  , 'min_child_weight': 18   \n",
    "                  , 'subsample': 0.74657   \n",
    "                  , 'colsample_bytree': 0.261858  \n",
    "                  , 'gamma': 0.57485               \n",
    "                  , 'seed': 1234\n",
    "                  , 'nthread': -1\n",
    "                  , 'silence': 1\n",
    "                  , 'eval_metric': 'auc'\n",
    "                  , 'scale_pos_weight': 1}\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "num_boost_round=10000\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=50)\n",
    "\n",
    "# [3393]\ttrain-auc:0.772549\teval-auc:0.728445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_iteration=1924\n",
    "clf_train = xgb.XGBClassifier(learning_rate = 0.01\n",
    "                  , n_estimators =best_xgb_iteration\n",
    "                  , max_depth = 4\n",
    "                  , min_child_weight = 18\n",
    "                  , subsample = 0.74657\n",
    "                  , colsample_bytree = 0.261858 \n",
    "                  , gamma = 0.57485\n",
    "                  , seed = 1234\n",
    "                  , nthread = -1\n",
    "                  , scale_pos_weight = 1\n",
    "                  )\n",
    "\n",
    "clf_train.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_ROC(gbm, dtrain, dvalid, dtest, y_train, y_valid, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print y_pred.max(), y_pred.min(), y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance=gbm.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance = pd.DataFrame(importance.items(), columns=['feature', 'fscore'])\n",
    "df_importance['fscore'] = df_importance['fscore'] / df_importance['fscore'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance.sort_values(['fscore'], ascending=False, inplace=True)\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32, 32))\n",
    "# df_importance.plot()\n",
    "df_importance[:20].plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.xlabel('relative importance')\n",
    "plt.gcf().savefig('feature_importance_xgb.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','wb') as pkl_file:\n",
    "    pickle.dump(gbm, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(gbm, \"pima.joblib.dat\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
